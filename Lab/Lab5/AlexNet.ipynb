{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ1Ec1UKezp1YSM3yFbO/N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me9jBhvVyN1o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8_rX6A1Cya6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_function():\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "  return loss_function"
      ],
      "metadata": {
        "id": "z0uH9Ilf0pa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "f4PQQXDD0w9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train function\n",
        "\n",
        "def train(net,data_loader,optimizer,loss_function, device='cuda:0'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "    net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader): \n",
        "      # Load data into GPU\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      outputs = net(inputs) # Forward pass\n",
        "      loss = loss_function(outputs,targets) # Apply the loss\n",
        "      loss.backward() # Backward pass\n",
        "      optimizer.step() # Update parameters\n",
        "      optimizer.zero_grad() # Reset the optimizer\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() \n",
        "      _, predicted = outputs.max(1) \n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item() \n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "lxSj5Ij609Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test function\n",
        "\n",
        "def test(net, data_loader, loss_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  net.eval() # Strictly needed if network contains layers which have different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "      _, predicted = outputs.max(1)\n",
        "      loss = loss_function(outputs, targets)\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item()\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "qWqtU76O1mOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data\n",
        "\n",
        "def get_data(batch_size, test_batch_size=256): \n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform = list() \n",
        "  transform.append(T.Resize((227,227)))\n",
        "  transform.append(T.ToTensor()) # Converts Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5])) # Normalizes the Tensors between [-1, 1]\n",
        "  transform = T.Compose(transform) # Composes the above transformations into one.\n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.CIFAR10('./data', train=True, transform=transform, download=True)\n",
        "  test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform, download=True) \n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data) \n",
        "  training_samples = int(num_samples*0.8+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, \n",
        "  validation_samples]) \n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "qEyVhoSQ1vd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Input arguments batch_size: Size of a mini-batch device: GPU where you want to train your\n",
        "network weight_decay: Weight decay co-efficient for regularization of weights momentum: Momentum\n",
        "for SGD optimizer epochs: Number of epochs for training the network '''\n",
        "\n",
        "def main(batch_size=128, device='cuda:0', learning_rate=0.01, weight_decay=0.000001, \n",
        "momentum=0.9, epochs=50):\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size) \n",
        "  net = AlexNet().to(device)\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum) \n",
        "  loss_function = get_loss_function() \n",
        "  for e in range(epochs): \n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, loss_function) \n",
        "    val_loss, val_accuracy = test(net, val_loader, loss_function) \n",
        "    print('Epoch: {:d}'.format(e+1)) \n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, \n",
        "    train_accuracy)) \n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, \n",
        "    val_accuracy)) \n",
        "    print('-----------------------------------------------------') \n",
        "    print('After training:') \n",
        "    train_loss, train_accuracy = test(net, train_loader, loss_function) \n",
        "    val_loss, val_accuracy = test(net, val_loader, loss_function) \n",
        "    test_loss, test_accuracy = test(net, test_loader, loss_function) \n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, \n",
        "    train_accuracy)) \n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, \n",
        "    val_accuracy)) \n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy)) \n",
        "    print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "AYDEiHW613G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5fQAB3xFdxQ",
        "outputId": "272d9ac7-1c2f-4283-8073-0387a1da77b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1\n",
            "\t Training loss 0.01142, Training accuracy 46.34\n",
            "\t Validation loss 0.00514, Validation accuracy 54.83\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00981, Training accuracy 55.26\n",
            "\t Validation loss 0.00514, Validation accuracy 54.83\n",
            "\t Test loss 0.00508, Test accuracy 55.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00812, Training accuracy 62.92\n",
            "\t Validation loss 0.00418, Validation accuracy 64.13\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00763, Training accuracy 66.59\n",
            "\t Validation loss 0.00418, Validation accuracy 64.13\n",
            "\t Test loss 0.00407, Test accuracy 65.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00670, Training accuracy 69.57\n",
            "\t Validation loss 0.00351, Validation accuracy 69.66\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00619, Training accuracy 72.83\n",
            "\t Validation loss 0.00351, Validation accuracy 69.66\n",
            "\t Test loss 0.00344, Test accuracy 70.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00580, Training accuracy 73.88\n",
            "\t Validation loss 0.00297, Validation accuracy 74.20\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00485, Training accuracy 78.83\n",
            "\t Validation loss 0.00297, Validation accuracy 74.20\n",
            "\t Test loss 0.00285, Test accuracy 75.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00516, Training accuracy 76.88\n",
            "\t Validation loss 0.00274, Validation accuracy 76.61\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00428, Training accuracy 81.44\n",
            "\t Validation loss 0.00274, Validation accuracy 76.61\n",
            "\t Test loss 0.00269, Test accuracy 76.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00458, Training accuracy 79.47\n",
            "\t Validation loss 0.00300, Validation accuracy 75.37\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00460, Training accuracy 80.22\n",
            "\t Validation loss 0.00300, Validation accuracy 75.37\n",
            "\t Test loss 0.00297, Test accuracy 75.22\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00417, Training accuracy 81.26\n",
            "\t Validation loss 0.00290, Validation accuracy 75.03\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00436, Training accuracy 80.77\n",
            "\t Validation loss 0.00290, Validation accuracy 75.03\n",
            "\t Test loss 0.00290, Test accuracy 75.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00377, Training accuracy 82.92\n",
            "\t Validation loss 0.00238, Validation accuracy 79.36\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00313, Training accuracy 86.04\n",
            "\t Validation loss 0.00238, Validation accuracy 79.36\n",
            "\t Test loss 0.00235, Test accuracy 79.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00343, Training accuracy 84.57\n",
            "\t Validation loss 0.00227, Validation accuracy 80.38\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00276, Training accuracy 88.17\n",
            "\t Validation loss 0.00227, Validation accuracy 80.38\n",
            "\t Test loss 0.00226, Test accuracy 80.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00316, Training accuracy 85.79\n",
            "\t Validation loss 0.00251, Validation accuracy 78.75\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00296, Training accuracy 86.55\n",
            "\t Validation loss 0.00251, Validation accuracy 78.75\n",
            "\t Test loss 0.00245, Test accuracy 79.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00285, Training accuracy 87.03\n",
            "\t Validation loss 0.00216, Validation accuracy 81.91\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00215, Training accuracy 91.06\n",
            "\t Validation loss 0.00216, Validation accuracy 81.91\n",
            "\t Test loss 0.00211, Test accuracy 82.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00260, Training accuracy 88.17\n",
            "\t Validation loss 0.00292, Validation accuracy 77.59\n",
            "-----------------------------------------------------\n",
            "After training:\n"
          ]
        }
      ]
    }
  ]
}